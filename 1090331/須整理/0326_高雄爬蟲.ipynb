{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823.3333333333333\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#首先，一樣要先透過requests.get連到該目標網址，然後一樣丟給了BeauitfulSoup去處理!\n",
    "resp = requests.get('http://jwlin.github.io/py-scraping-analysis-book/ch2/table/table.html')\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "# 計算課程均價\n",
    "# 取得所有課程價錢: 方法一, 使用 index\n",
    "prices = []\n",
    "#tr就跟row一樣，所以先取tr資料\n",
    "#然後就透過迴圈去把所有tr的價格資料取出，價格td在第三欄，以index來計算的話是2。\n",
    "rows = soup.find('table', 'table').tbody.find_all('tr')\n",
    "for row in rows:\n",
    "    price = row.find_all('td')[2].text\n",
    "    prices.append(int(price))\n",
    "print(sum(prices)/len(prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823.3333333333333\n"
     ]
    }
   ],
   "source": [
    "# 取得所有課程價錢: 方法二, <a> 的 parent (<td>) 的 previous_sibling\n",
    "prices = []\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    price = link.parent.previous_sibling.text\n",
    "    #.parent(父).previous_sibling(兄) \n",
    "    prices.append(int(price))\n",
    "print(sum(prices) / len(prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['初心者 - Python入門', '初學者', '1490']\n",
      "['Python 網頁爬蟲入門實戰', '有程式基礎的初學者', '1890']\n",
      "['Python 機器學習入門實戰 (預計)', '有程式基礎的初學者', '1890']\n",
      "['Python 資料科學入門實戰 (預計)', '有程式基礎的初學者', '1890']\n",
      "['Python 資料視覺化入門實戰 (預計)', '有程式基礎的初學者', '1890']\n",
      "['Python 網站架設入門實戰 (預計)', '有程式基礎的初學者', '1890']\n"
     ]
    }
   ],
   "source": [
    "# 取得每一列所有欄位文字資訊: stripped_strings\n",
    "rows = soup.find('table', 'table').tbody.find_all('tr')\n",
    "for row in rows:\n",
    "    print([s for s in row.stripped_strings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初心者 - Python入門 初學者 1490 http://www.pycone.com img/python-logo.png\n",
      "Python 網頁爬蟲入門實戰 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 機器學習入門實戰 (預計) 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 資料科學入門實戰 (預計) 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 資料視覺化入門實戰 (預計) 有程式基礎的初學者 1890 http://www.pycone.com img/python-logo.png\n",
      "Python 網站架設入門實戰 (預計) 有程式基礎的初學者 1890 None img/python-logo.png\n"
     ]
    }
   ],
   "source": [
    "# 取得每一列所有欄位資訊: find_all('td') or row.children\n",
    "rows = soup.find('table', 'table').tbody.find_all('tr')\n",
    "for row in rows:\n",
    "    all_tds = row.find_all('td')  \n",
    "    # 方法一: find_all('td')\n",
    "    # all_tds = [td for td in row.children]  \n",
    "    # 方法二: 找出 row (tr) 所有的直接 (下一層) children\n",
    "    # 以下執行時會報錯, 因為最後一列的 <a> 沒有 'href' 屬性\n",
    "    # print(all_tds[0].text, all_tds[1].text, all_tds[2].text, all_tds[3].a['href'], all_tds[3].a.img['src'])\n",
    "    # 取得 href 屬性前先檢查其是否存在\n",
    "    if 'href' in all_tds[3].a.attrs:\n",
    "        href = all_tds[3].a['href']\n",
    "    else:\n",
    "        href = None\n",
    "    print(all_tds[0].text, all_tds[1].text, all_tds[2].text, href, all_tds[3].a.img['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div data-num=\"123\">foo!</div>\n"
     ]
    }
   ],
   "source": [
    "#attrs屬性\n",
    "html = '<div data-num=\"123\">foo!</div>'\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "data_tag = soup.find(attrs={\"data-num\": \"123\"})\n",
    "print(data_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'expectation': '87%', 'ch_name': '記憶屋', 'eng_name': 'kiokuya', 'movie_id': '10470', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/January2020/V6MY1SapAqTFmjnS4ChM-800x1138.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '89%', 'ch_name': 'Re從零開始的異世界生活 外傳集', 'eng_name': '', 'movie_id': '10480', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/January2020/TDRN5rFEzoyTnUEnQ6EN-500x720.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '88%', 'ch_name': '第六感生死戀', 'eng_name': 'Ghost', 'movie_id': '10504', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2020/ygqvTlkSMVgefHs3LplV-1000x1424.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '81%', 'ch_name': '抓住救命稻草的野獸們', 'eng_name': 'Beasts Clawing at Straws', 'movie_id': '10507', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/February2020/uxiNS7oVZZ92gPsGXJl5-1280x1834.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '85%', 'ch_name': '最初的晚餐', 'eng_name': 'The First Supper', 'movie_id': '10510', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2020/fepasHu2m4U1yL2Vt0d5-1920x2743.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '96%', 'ch_name': '鏡中驚魂', 'eng_name': 'Look Away', 'movie_id': '10514', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/February2020/bu1HJ1AY8lgGskKvRNjo-760x1280.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '94%', 'ch_name': '無聲救援', 'eng_name': 'Resistance', 'movie_id': '10521', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/February2020/j5oB00zSjiNcyTWR1xa0-504x720.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '82%', 'ch_name': '迷雁返家路', 'eng_name': 'Spread Your Wings', 'movie_id': '10533', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/February2020/vEPtfrJ0MXMA8K7Ux67H-504x720.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '84%', 'ch_name': '託陰2：布拉姆回來了', 'eng_name': 'Brahms: The Boy II', 'movie_id': '10540', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2020/UwajHV3G3AnIMOUNAoCv-504x720.jpg', 'release_date': '2020-03-27'}\n",
      "{'expectation': '97%', 'ch_name': '絕命直播', 'eng_name': 'Line of Duty', 'movie_id': '10548', 'poster_url': 'https://movies.yahoo.com.tw/x/r/w420/i/o/production/movies/March2020/ZfXP83HaFCyxjVnKu1HI-2756x3935.jpg', 'release_date': '2020-03-27'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "Y_MOVIE_URL = 'https://tw.movies.yahoo.com/movie_thisweek.html'\n",
    "def get_web_page(url):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url:', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "def get_date(date_str):\n",
    "    # e.g. \"上映日期：2017-03-23\" -> match.group(0): \"2017-03-23\"\n",
    "    pattern = '\\d+-\\d+-\\d+'\n",
    "    match = re.search(pattern, date_str)\n",
    "    if match is None:\n",
    "        return date_str\n",
    "    else:\n",
    "        return match.group(0)\n",
    "\n",
    "\n",
    "def get_movie_id(url):\n",
    "    # e.g., 'https://movies.yahoo.com.tw/movieinfo_main/%E6%AD%BB%E4%BE%8D2-deadpool-2-7820.html\n",
    "    try:\n",
    "        movie_id = url.split('.html')[0].split('-')[-1]\n",
    "    except:\n",
    "        movie_id = url\n",
    "    return movie_id\n",
    "def get_movies(dom):\n",
    "    soup = BeautifulSoup(dom, 'html.parser')\n",
    "    movies = []\n",
    "    rows = soup.find_all('div', 'release_info_text')\n",
    "    for row in rows:\n",
    "        movie = dict()\n",
    "        movie['expectation'] = row.find('div', 'leveltext').span.text.strip()\n",
    "        movie['ch_name'] = row.find('div', 'release_movie_name').a.text.strip()\n",
    "        movie['eng_name'] = row.find('div', 'release_movie_name').find('div', 'en').a.text.strip()\n",
    "        movie['movie_id'] = get_movie_id(row.find('div', 'release_movie_name').a['href'])\n",
    "        movie['poster_url'] = row.parent.find_previous_sibling('div', 'release_foto').a.img['src']\n",
    "        movie['release_date'] = get_date(row.find('div', 'release_movie_time').text)\n",
    "        movies.append(movie)\n",
    "    return movies\n",
    "if __name__ == '__main__':\n",
    "    page = get_web_page(Y_MOVIE_URL)\n",
    "    movies = get_movies(page)\n",
    "    for movie in movies:\n",
    "        print(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': '富貴角', 'pm25': '15', 'pm25_1': '18'}, {'name': '萬里', 'pm25': '', 'pm25_1': '21'}, {'name': '淡水', 'pm25': '21', 'pm25_1': '23'}, {'name': '林口', 'pm25': '36', 'pm25_1': '30'}, {'name': '三重', 'pm25': '29', 'pm25_1': '29'}, {'name': '菜寮', 'pm25': '26', 'pm25_1': '25'}, {'name': '汐止', 'pm25': '21', 'pm25_1': '22'}, {'name': '新莊', 'pm25': '28', 'pm25_1': '27'}, {'name': '永和', 'pm25': '24', 'pm25_1': '24'}, {'name': '板橋', 'pm25': '18', 'pm25_1': '23'}, {'name': '土城', 'pm25': '34', 'pm25_1': '20'}, {'name': '新店', 'pm25': '21', 'pm25_1': '25'}, {'name': '陽明', 'pm25': '28', 'pm25_1': '26'}, {'name': '士林', 'pm25': '17', 'pm25_1': '24'}, {'name': '大同', 'pm25': '21', 'pm25_1': '22'}, {'name': '中山', 'pm25': '26', 'pm25_1': '29'}, {'name': '松山', 'pm25': '28', 'pm25_1': '25'}, {'name': '萬華', 'pm25': '23', 'pm25_1': '30'}, {'name': '古亭', 'pm25': '29', 'pm25_1': '26'}, {'name': '基隆', 'pm25': '16', 'pm25_1': '13'}, {'name': '大園', 'pm25': '27', 'pm25_1': '31'}, {'name': '觀音', 'pm25': '19', 'pm25_1': '22'}, {'name': '桃園', 'pm25': '37', 'pm25_1': '33'}, {'name': '平鎮', 'pm25': '37', 'pm25_1': '35'}, {'name': '中壢', 'pm25': '36', 'pm25_1': '42'}, {'name': '龍潭', 'pm25': '34', 'pm25_1': '40'}]\n",
      "請輸入地區 ? (例如:林口, 桃園) : 林口\n",
      "{'name': '林口', 'pm25': '36', 'pm25_1': '30'}\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def append_list_pm25():\n",
    "    url = 'https://taqm.epa.gov.tw/pm25/tw/PM25A.aspx?area=1'\n",
    "    html = requests.get(url)\n",
    "    sp = BeautifulSoup(html.text, 'html.parser')\n",
    "    rs = sp.find_all(\"tr\", {\"align\": \"center\", \"style\": \"border-width:1px;border-style:Solid;\"})\n",
    "    for r in rs:\n",
    "        name = r.find('a')\n",
    "        pm25 = r.find_all('span')\n",
    "        dic = {}\n",
    "        dic.setdefault('name',   name.text.strip())\n",
    "        dic.setdefault('pm25',   pm25[0].text.strip())\n",
    "        dic.setdefault('pm25_1', pm25[1].text.strip())\n",
    "        list.append(dic)\n",
    "\n",
    "def get_pm25(name):\n",
    "    for d in list:\n",
    "        if d.get('name') == name:\n",
    "            return d\n",
    "\n",
    "list = []\n",
    "append_list_pm25()\n",
    "print(list)\n",
    "\n",
    "name = input('請輸入地區 ? (例如:林口, 桃園) : ')\n",
    "d = get_pm25(name)\n",
    "print(d)\n",
    "print(d.get('pm25'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
